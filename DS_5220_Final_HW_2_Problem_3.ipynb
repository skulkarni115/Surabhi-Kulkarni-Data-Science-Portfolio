{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "caF4fCGDw9JD"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import sklearn\n",
        "from scipy import spatial\n",
        "from statistics import mode\n",
        "import sklearn.metrics.pairwise\n",
        "from statistics import mean\n",
        "import random\n",
        "from collections import defaultdict\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "from scipy.stats import multivariate_normal"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Need to read in the data here\n",
        "with open('2gaussian.txt') as gauss_file:\n",
        "  all_lines = gauss_file.readlines()\n",
        "  total_data = []\n",
        "#print(all_lines)\n",
        "for line in all_lines:\n",
        "  line_list = line.strip('\\n').split(' ')\n",
        "  new_list = []\n",
        "  for i in line_list:\n",
        "    new_list.append(float(i))\n",
        "  total_data.append(new_list)"
      ],
      "metadata": {
        "id": "5fyhuVfyw_uM"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Generate random initial values\n",
        "k = 2\n",
        "in_cluster = random.sample(range(0,6000),k)\n",
        "total_data = np.array(total_data)\n",
        "#Those become the centroids of the two points\n",
        "cluster_means = []\n",
        "for i in range(len(in_cluster)):\n",
        "  cluster_means.append(total_data[in_cluster[i]])\n",
        "cluster_means = np.array(cluster_means)\n",
        "mean1 = cluster_means[0]\n",
        "mean2 = cluster_means[1]\n",
        "\n",
        "\n",
        "#Calculate covariance of all assignments (Needs to change in the while loop), (# of gaussian mixtures times the dimensions fo the data set (shape will be 2,2,2))\n",
        "cov_data1 = []\n",
        "cov_data2 = []\n",
        "for i in range(len(total_data)):\n",
        "  cov1 = np.cov(total_data[i], cluster_means[0])\n",
        "  cov2 = np.cov(total_data[i], cluster_means[1])\n",
        "  cov_data1.append(cov1)\n",
        "  cov_data2.append(cov2)\n",
        "\n",
        "\n",
        "prob_list = []\n",
        "for i in range(6000):\n",
        "  prob1 = random.uniform(0,1)\n",
        "  prob2 = 1-prob1\n",
        "  probs = []\n",
        "  probs.append(prob1)\n",
        "  probs.append(prob2)\n",
        "  prob_list.append(probs)\n",
        "\n",
        "prob_list = np.array(prob_list)\n",
        "covk1 = sum(cov_data1*prob_list[0])/sum(prob_list[0])\n",
        "covk2 = sum(cov_data2*prob_list[1])/sum(prob_list[1])\n",
        "\n",
        "#Calculate weights (pick two probabilities that add up to 1)\n",
        "weights = [.5,.5]\n",
        "\n",
        "#Begin while loop here\n",
        "\n",
        "iter = 0\n",
        "\n",
        "while(iter <10):\n",
        "\n",
        "#E Step\n",
        "#Calculate probabilities (p(x1|k)\n",
        "  model1 = multivariate_normal(mean1, covk1, allow_singular = True)\n",
        "  model2 = multivariate_normal(mean2, covk2, allow_singular = True)\n",
        "  prob_list = []\n",
        "  for i in range(len(total_data)):\n",
        "     prob1 = model1.pdf(total_data[i])\n",
        "     prob2 = model2.pdf(total_data[i])\n",
        "     probs = []\n",
        "     probs.append(prob1)\n",
        "     probs.append(prob2)\n",
        "     prob_list.append(probs)\n",
        "  prob_list = np.array(prob_list)\n",
        "\n",
        "   #Calculate pik\n",
        "   #x times probabilities (All elements)/sum of prob in the cluster]\n",
        "   #Need to normalize the piks\n",
        "  pik = []\n",
        "  for i in range(len(total_data)):\n",
        "    piks = []\n",
        "    new_pik1 = (prob_list[i][0]* weights[0])\n",
        "    new_pik2 = (prob_list[i][1]* weights[1])\n",
        "    piks.append(new_pik1)\n",
        "    piks.append(new_pik2)\n",
        "    pik.append(piks)\n",
        "  #normalization of piks\n",
        "  pik = np.array(pik)\n",
        "  pik_sum = np.sum(pik,axis = 1)\n",
        "  for i in range(len(pik)):\n",
        "    pik[i] = pik[i]/pik_sum[i]\n",
        "\n",
        "  old_mean1 = mean1\n",
        "  old_mean2 = mean2\n",
        "  #Mstep\n",
        "  #Means\n",
        "  mean1_data = []\n",
        "  mean2_data = []\n",
        "  pik1_sum = sum(pik[0])\n",
        "  pik2_sum = sum(pik[1])\n",
        "  for i in range(len(total_data)):\n",
        "    mean1 = pik[i][0] * total_data[i]\n",
        "    mean2 = pik[i][1] * total_data[i]\n",
        "    mean1_data.append(mean1)\n",
        "    mean2_data.append(mean2)\n",
        "  mean1 = sum(mean1_data)/np.sum(pik, axis = 0)\n",
        "  mean2 = sum(mean2_data)/np.sum(pik, axis = 0)\n",
        "\n",
        "\n",
        "  if np.array_equal(old_mean1,mean1) and np.array_equal(old_mean2,mean2):\n",
        "    break\n",
        "\n",
        "  #covariance\n",
        "  cov_data1 = []\n",
        "  cov_data2 = []\n",
        "  for i in range(k):\n",
        "    diff1 = (total_data - mean1).T\n",
        "    diff2 = (total_data-mean2).T\n",
        "    new_pik_sum = np.sum(pik, axis = 0)\n",
        "    w1_sum = np.dot(pik[:,0]*diff1,diff1.T)\n",
        "    covk1 = w1_sum/new_pik_sum[0]\n",
        "    w2_sum = np.dot(pik[:,1]*diff2,diff2.T)\n",
        "    covk2 = w2_sum/new_pik_sum[1]\n",
        "\n",
        "\n",
        "\n",
        "  #Weights\n",
        "  weights = []\n",
        "  w1 = 0\n",
        "  w2 = 0\n",
        "  for i in range(len(pik)):\n",
        "    w1 += pik[i][0]\n",
        "    w2 += pik[i][1]\n",
        "  weights.append(w1/6000)\n",
        "  weights.append(w2/6000)\n",
        "  \n",
        "  iter+=1"
      ],
      "metadata": {
        "id": "wU4kANuSxCuY"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('means1',mean1)\n",
        "print('means2',mean2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ucLnHBen7Chs",
        "outputId": "3e578077-8e1b-4940-d2d0-6314d4e3b4b1"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "means1 [7.19246972 5.42523813]\n",
            "means2 [2.82509076 3.02971859]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3 Gaussian Data- GMM Implementation"
      ],
      "metadata": {
        "id": "UgO-nKRieLlr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Need to read in the data here\n",
        "with open('3gaussian.txt') as gauss_file:\n",
        "  all_lines = gauss_file.readlines()\n",
        "  total_data = []\n",
        "#print(all_lines)\n",
        "for line in all_lines:\n",
        "  line_list = line.strip('\\n').split(' ')\n",
        "  new_list = []\n",
        "  for i in line_list:\n",
        "    new_list.append(float(i))\n",
        "  total_data.append(new_list)"
      ],
      "metadata": {
        "id": "XaNC5PT3eS9N"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Generate random initial values\n",
        "k = 3\n",
        "in_cluster = random.sample(range(0,6000),k)\n",
        "total_data = np.array(total_data)\n",
        "#Those become the centroids of the two points\n",
        "cluster_means = []\n",
        "for i in range(len(in_cluster)):\n",
        "  cluster_means.append(total_data[in_cluster[i]])\n",
        "cluster_means = np.array(cluster_means)\n",
        "mean1 = cluster_means[0]\n",
        "mean2 = cluster_means[1]\n",
        "mean3 = cluster_means[2]\n",
        "\n",
        "\n",
        "#Calculate covariance of all assignments (Needs to change in the while loop), (# of gaussian mixtures times the dimensions fo the data set (shape will be 2,2,2))\n",
        "cov_data1 = []\n",
        "cov_data2 = []\n",
        "cov_data3 = []\n",
        "for i in range(len(total_data)):\n",
        "  cov1 = np.cov(total_data[i], cluster_means[0])\n",
        "  cov2 = np.cov(total_data[i], cluster_means[1])\n",
        "  cov3 = np.cov(total_data[i],cluster_means[2])\n",
        "  cov_data1.append(cov1)\n",
        "  cov_data2.append(cov2)\n",
        "  cov_data3.append(cov3)\n",
        "\n",
        "\n",
        "prob_list = []\n",
        "for i in range(10000):\n",
        "  prob1 = random.uniform(0,1)\n",
        "  prob2 = (1-prob1)/2\n",
        "  prob3 = prob2\n",
        "  probs = []\n",
        "  probs.append(prob1)\n",
        "  probs.append(prob2)\n",
        "  probs.append(prob3)\n",
        "  prob_list.append(probs)\n",
        "\n",
        "prob_list = np.array(prob_list)\n",
        "covk1 = sum(cov_data1*prob_list[0][0:1])/sum(prob_list[0])\n",
        "covk2 = sum(cov_data2*prob_list[1][0:1])/sum(prob_list[1])\n",
        "covk3 = sum(cov_data3*prob_list[2][0:1])/sum(prob_list[2])\n",
        "\n",
        "#Calculate weights (pick two probabilities that add up to 1)\n",
        "weights = [1/3,1/3,1/3]\n",
        "\n",
        "#Begin while loop here\n",
        "\n",
        "iter = 0\n",
        "\n",
        "while(True):\n",
        "\n",
        "#E Step\n",
        "#Calculate probabilities (p(x1|k)\n",
        "  model1 = multivariate_normal(mean1, covk1, allow_singular = True)\n",
        "  model2 = multivariate_normal(mean2, covk2, allow_singular = True)\n",
        "  model3 = multivariate_normal(mean3, covk3, allow_singular = True)\n",
        "  prob_list = []\n",
        "  for i in range(len(total_data)):\n",
        "     prob1 = model1.pdf(total_data[i])\n",
        "     prob2 = model2.pdf(total_data[i])\n",
        "     prob3 = model3.pdf(total_data[i])\n",
        "     probs = []\n",
        "     probs.append(prob1)\n",
        "     probs.append(prob2)\n",
        "     probs.append(prob3)\n",
        "     prob_list.append(probs)\n",
        "  prob_list = np.array(prob_list)\n",
        "\n",
        "   #Calculate pik\n",
        "   #x times probabilities (All elements)/sum of prob in the cluster]\n",
        "   #Need to normalize the piks\n",
        "  pik = []\n",
        "  for i in range(len(total_data)):\n",
        "    piks = []\n",
        "    new_pik1 = (prob_list[i][0]* weights[0])\n",
        "    new_pik2 = (prob_list[i][1]* weights[1])\n",
        "    new_pik3 = (prob_list[i][2]* weights[2])\n",
        "    piks.append(new_pik1)\n",
        "    piks.append(new_pik2)\n",
        "    piks.append(new_pik3)\n",
        "    pik.append(piks)\n",
        "  #normalization of piks\n",
        "  pik = np.array(pik)\n",
        "  pik_sum = np.sum(pik,axis = 1)\n",
        "  for i in range(len(pik)):\n",
        "    pik[i] = pik[i]/pik_sum[i]\n",
        "\n",
        "  old_mean1 = mean1\n",
        "  old_mean2 = mean2\n",
        "  old_mean3 = mean3\n",
        "  #Mstep\n",
        "  #Means\n",
        "  mean1_data = []\n",
        "  mean2_data = []\n",
        "  mean3_data = []\n",
        "  pik1_sum = sum(pik[0])\n",
        "  pik2_sum = sum(pik[1])\n",
        "  pik3_sum = sum(pik[2])\n",
        "  for i in range(len(total_data)):\n",
        "    mean1 = pik[i][0] * total_data[i]\n",
        "    mean2 = pik[i][1] * total_data[i]\n",
        "    mean3 = pik[i][2] * total_data[i]\n",
        "    mean1_data.append(mean1)\n",
        "    mean2_data.append(mean2)\n",
        "    mean3_data.append(mean3)\n",
        "  mean1 = sum(mean1_data)/np.sum(pik, axis = 0)[0]\n",
        "  mean2 = sum(mean2_data)/np.sum(pik, axis = 0)[1]\n",
        "  mean3 = sum(mean3_data)/np.sum(pik, axis = 0)[2]\n",
        "\n",
        "\n",
        "  if np.array_equal(old_mean1,mean1) and np.array_equal(old_mean2,mean2) and np.array_equal(old_mean3,mean3):\n",
        "    break\n",
        "\n",
        "  #covariance\n",
        "  cov_data1 = []\n",
        "  cov_data2 = []\n",
        "  cov_data3\n",
        "  for i in range(k):\n",
        "    diff1 = (total_data - mean1).T\n",
        "    diff2 = (total_data-mean2).T\n",
        "    diff3 = (total_data-mean3).T\n",
        "    new_pik_sum = np.sum(pik, axis = 0)\n",
        "    w1_sum = np.dot(pik[:,0]*diff1,diff1.T)\n",
        "    covk1 = w1_sum/new_pik_sum[0]\n",
        "    w2_sum = np.dot(pik[:,1]*diff2,diff2.T)\n",
        "    covk2 = w2_sum/new_pik_sum[1]\n",
        "    w3_sum = np.dot(pik[:,2]*diff3,diff3.T)\n",
        "    covk3 = w3_sum/new_pik_sum[2]\n",
        "\n",
        "\n",
        "\n",
        "  #Weights\n",
        "  weights = []\n",
        "  w1 = 0\n",
        "  w2 = 0\n",
        "  w3 = 0\n",
        "  for i in range(len(pik)):\n",
        "    w1 += pik[i][0]\n",
        "    w2 += pik[i][1]\n",
        "    w3 += pik[i][2]\n",
        "  weights.append(w1/10000)\n",
        "  weights.append(w2/10000)\n",
        "  weights.append(w3/10000)\n",
        "  \n",
        "  iter+=1"
      ],
      "metadata": {
        "id": "CKJHFabPegQ3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('mean1',mean1)\n",
        "print('mean2',mean2)\n",
        "print('mean3',mean3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DzEZjty6gVOr",
        "outputId": "476d98d0-0b2c-4c2f-9d58-85a1af6e2802"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mean1 [3.03975701 3.04866909]\n",
            "mean2 [5.01175547 7.00149623]\n",
            "mean3 [7.02157607 4.01546857]\n"
          ]
        }
      ]
    }
  ]
}